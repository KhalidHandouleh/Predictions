{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Post Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "\n",
    "**This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).**\n",
    "\n",
    "- The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. \n",
    "- The original content be publicly accessed and retrieved using the provided urls. \n",
    "- Acquisition date: January 8, 2015 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features description\n",
    "\n",
    "**Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 target)**\n",
    "\n",
    "Attribute Information: \n",
    "\n",
    "0. url: URL of the article (non-predictive) \n",
    "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) \n",
    "2. n_tokens_title: Number of words in the title \n",
    "3. n_tokens_content: Number of words in the content \n",
    "4. n_unique_tokens: Rate of unique words in the content \n",
    "5. n_non_stop_words: Rate of non-stop words in the content \n",
    "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content \n",
    "7. num_hrefs: Number of links \n",
    "8. num_self_hrefs: Number of links to other articles published by Mashable \n",
    "9. num_imgs: Number of images \n",
    "10. num_videos: Number of videos \n",
    "11. average_token_length: Average length of the words in the content \n",
    "12. num_keywords: Number of keywords in the metadata \n",
    "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? \n",
    "14. data_channel_is_entertainment: Is data channel 'Entertainment'? \n",
    "15. data_channel_is_bus: Is data channel 'Business'? \n",
    "16. data_channel_is_socmed: Is data channel 'Social Media'? \n",
    "17. data_channel_is_tech: Is data channel 'Tech'? \n",
    "18. data_channel_is_world: Is data channel 'World'? \n",
    "19. kw_min_min: Worst keyword (min. shares) \n",
    "20. kw_max_min: Worst keyword (max. shares) \n",
    "21. kw_avg_min: Worst keyword (avg. shares) \n",
    "22. kw_min_max: Best keyword (min. shares) \n",
    "23. kw_max_max: Best keyword (max. shares) \n",
    "24. kw_avg_max: Best keyword (avg. shares) \n",
    "25. kw_min_avg: Avg. keyword (min. shares) \n",
    "26. kw_max_avg: Avg. keyword (max. shares) \n",
    "27. kw_avg_avg: Avg. keyword (avg. shares) \n",
    "28. self_reference_min_shares: Min. shares of referenced articles in Mashable \n",
    "29. self_reference_max_shares: Max. shares of referenced articles in Mashable \n",
    "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable \n",
    "31. weekday_is_monday: Was the article published on a Monday? \n",
    "32. weekday_is_tuesday: Was the article published on a Tuesday? \n",
    "33. weekday_is_wednesday: Was the article published on a Wednesday? \n",
    "34. weekday_is_thursday: Was the article published on a Thursday? \n",
    "35. weekday_is_friday: Was the article published on a Friday? \n",
    "36. weekday_is_saturday: Was the article published on a Saturday? \n",
    "37. weekday_is_sunday: Was the article published on a Sunday? \n",
    "38. is_weekend: Was the article published on the weekend? \n",
    "39. LDA_00: Closeness to LDA topic 0 \n",
    "40. LDA_01: Closeness to LDA topic 1 \n",
    "41. LDA_02: Closeness to LDA topic 2 \n",
    "42. LDA_03: Closeness to LDA topic 3 \n",
    "43. LDA_04: Closeness to LDA topic 4 \n",
    "44. global_subjectivity: Text subjectivity \n",
    "45. global_sentiment_polarity: Text sentiment polarity \n",
    "46. global_rate_positive_words: Rate of positive words in the content \n",
    "47. global_rate_negative_words: Rate of negative words in the content \n",
    "48. rate_positive_words: Rate of positive words among non-neutral tokens \n",
    "49. rate_negative_words: Rate of negative words among non-neutral tokens \n",
    "50. avg_positive_polarity: Avg. polarity of positive words \n",
    "51. min_positive_polarity: Min. polarity of positive words \n",
    "52. max_positive_polarity: Max. polarity of positive words \n",
    "53. avg_negative_polarity: Avg. polarity of negative words \n",
    "54. min_negative_polarity: Min. polarity of negative words \n",
    "55. max_negative_polarity: Max. polarity of negative words \n",
    "56. title_subjectivity: Title subjectivity \n",
    "57. title_sentiment_polarity: Title polarity \n",
    "58. abs_title_subjectivity: Absolute subjectivity level \n",
    "59. abs_title_sentiment_polarity: Absolute polarity level \n",
    "60. shares: Number of shares (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>timedelta</th>\n      <th>n_tokens_title</th>\n      <th>n_tokens_content</th>\n      <th>n_unique_tokens</th>\n      <th>n_non_stop_words</th>\n      <th>n_non_stop_unique_tokens</th>\n      <th>num_hrefs</th>\n      <th>num_self_hrefs</th>\n      <th>num_imgs</th>\n      <th>...</th>\n      <th>min_positive_polarity</th>\n      <th>max_positive_polarity</th>\n      <th>avg_negative_polarity</th>\n      <th>min_negative_polarity</th>\n      <th>max_negative_polarity</th>\n      <th>title_subjectivity</th>\n      <th>title_sentiment_polarity</th>\n      <th>abs_title_subjectivity</th>\n      <th>abs_title_sentiment_polarity</th>\n      <th>shares</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n      <td>731.0</td>\n      <td>12.0</td>\n      <td>219.0</td>\n      <td>0.663594</td>\n      <td>1.0</td>\n      <td>0.815385</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.100000</td>\n      <td>0.7</td>\n      <td>-0.350000</td>\n      <td>-0.600</td>\n      <td>-0.200000</td>\n      <td>0.500000</td>\n      <td>-0.187500</td>\n      <td>0.000000</td>\n      <td>0.187500</td>\n      <td>593</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n      <td>731.0</td>\n      <td>9.0</td>\n      <td>255.0</td>\n      <td>0.604743</td>\n      <td>1.0</td>\n      <td>0.791946</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.033333</td>\n      <td>0.7</td>\n      <td>-0.118750</td>\n      <td>-0.125</td>\n      <td>-0.100000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>711</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n      <td>731.0</td>\n      <td>9.0</td>\n      <td>211.0</td>\n      <td>0.575130</td>\n      <td>1.0</td>\n      <td>0.663866</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.100000</td>\n      <td>1.0</td>\n      <td>-0.466667</td>\n      <td>-0.800</td>\n      <td>-0.133333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>1500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n      <td>731.0</td>\n      <td>9.0</td>\n      <td>531.0</td>\n      <td>0.503788</td>\n      <td>1.0</td>\n      <td>0.665635</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.136364</td>\n      <td>0.8</td>\n      <td>-0.369697</td>\n      <td>-0.600</td>\n      <td>-0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>1200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n      <td>731.0</td>\n      <td>13.0</td>\n      <td>1072.0</td>\n      <td>0.415646</td>\n      <td>1.0</td>\n      <td>0.540890</td>\n      <td>19.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>...</td>\n      <td>0.033333</td>\n      <td>1.0</td>\n      <td>-0.220192</td>\n      <td>-0.500</td>\n      <td>-0.050000</td>\n      <td>0.454545</td>\n      <td>0.136364</td>\n      <td>0.045455</td>\n      <td>0.136364</td>\n      <td>505</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# importing data\n",
    "data_path= './data/OnlineNewsPopularity.csv'\n",
    "articles = pd.read_csv(data_path)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(38273, 59)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Remove whitespace\n",
    "articles = articles.rename(columns=lambda x: x.strip())\n",
    "# Deleting non predictive columns\n",
    "non_predictive = ['url', 'timedelta']\n",
    "articles.drop(non_predictive, axis=1, inplace=True)\n",
    "# Removing articles with 'extreme' values\n",
    "articles = articles[(articles['shares'] > 100) & (articles['shares'] < 15000)]\n",
    "# the target is extremely skewed because there are some posts that are very popular. \n",
    "# We will apply a logaritmic transformation to this feature to address this issue.\n",
    "articles['shares'] = np.log(articles['shares'])\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_name = 'shares'\n",
    "robust_scaler = RobustScaler()\n",
    "X = articles.drop('shares', axis=1)\n",
    "X = robust_scaler.fit_transform(X)\n",
    "y = articles[target_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=124)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a DataFrame for model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame(index=['train_mse','test_mse'], \n",
    "                          columns=['NULL', 'MLR', 'KNN', 'LASSO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Null model: always predict the average of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_null = y_train.mean()\n",
    "\n",
    "models.loc['train_mse','NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null, y_train.size), \n",
    "                                                   y_true=y_train)\n",
    "\n",
    "models.loc['test_mse','NULL'] = mean_squared_error(y_pred=np.repeat(y_pred_null, y_test.size), \n",
    "                                                   y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import the estimator object (model)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# 2. Create an instance of the estimator\n",
    "linear_regression = LinearRegression()\n",
    "# 3. Use the trainning data to train the estimator\n",
    "linear_regression.fit(X_train, y_train)\n",
    "# 4. Evaluate the model\n",
    "models.loc['train_mse','MLR'] = mean_squared_error(y_pred=linear_regression.predict(X_train), \n",
    "                                                    y_true=y_train)\n",
    "\n",
    "models.loc['test_mse','MLR'] = mean_squared_error(y_pred=linear_regression.predict(X_test), \n",
    "                                                   y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. K-Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import the estimator object (model)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# 2. Create an instance of the estimator\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='distance', metric='euclidean', n_jobs=-1)\n",
    "# 3. Use the training data to train the estimator\n",
    "knn.fit(X_train, y_train)\n",
    "# 4. Evaluate the model\n",
    "models.loc['train_mse','KNN'] = mean_squared_error(y_pred=knn.predict(X_train), \n",
    "                                                    y_true=y_train)\n",
    "\n",
    "models.loc['test_mse','KNN'] = mean_squared_error(y_pred=knn.predict(X_test), \n",
    "                                                   y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the estimator object (model)\n",
    "from sklearn.linear_model import Lasso\n",
    "# 2. Create an instance of the estimator\n",
    "lasso = Lasso(alpha=0.05)\n",
    "# 3. Use the training data to train the estimator\n",
    "lasso.fit(X_train, y_train)\n",
    "# 4. Evaluate the model\n",
    "models.loc['train_mse','LASSO'] = mean_squared_error(y_pred=lasso.predict(X_train), \n",
    "                                                    y_true=y_train)\n",
    "\n",
    "models.loc['test_mse','LASSO'] = mean_squared_error(y_pred=lasso.predict(X_test), \n",
    "                                                   y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "models.T.plot(kind='barh', ax=ax)\n",
    "ax.set_title('MSE for Regression Models Using All Features')\n",
    "ax.legend(loc=8, ncol=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.scatter(linear_regression.predict(X_test), y_test, s=4)\n",
    "ax.plot(y_test, y_test, color='red')\n",
    "ax.set_title('MRL all features: predictions vs. observed values (test data)')\n",
    "ax.set_xlabel('Predicted target values')\n",
    "ax.set_ylabel('Testing target values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Citation**    \n",
    "    K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\n",
    "    Support System for Predicting the Popularity of Online News. Proceedings\n",
    "    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\n",
    "    September, Coimbra, Portugal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}